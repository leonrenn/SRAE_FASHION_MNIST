{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder for Super Resolution on Fashion MNIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This AutoEncoder (AE) will try to remove the random noise on the pictures of clothings.<br>\n",
    "The input data is the famous Fahsion MNIST with thousands of 28x28 pictures. <br>\n",
    "Gaussian noise was added on the input data. <br>\n",
    "The target images are the original images from the dataset.<br>\n",
    "The input to the AE are the pictures that with additional noise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of images and analytic cruves\n",
    "import matplotlib.pyplot as plt\n",
    "# analytics\n",
    "import numpy as np\n",
    "# dl framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "# accelerated dl framework\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer import Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data and Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download full dataset for trainin and validation\n",
    "mnist_dataset = torchvision.datasets.FashionMNIST(root=\"\",\n",
    "                                                  transform=torchvision.transforms.Compose([\n",
    "                                                  torchvision.transforms.ToTensor(),\n",
    "                                                  torchvision.transforms.Normalize((0.5,), (0.5,))]),\n",
    "                                                  train=True,\n",
    "                                                  download=True)\n",
    "\n",
    "# split the entire dataset into validation and training set\n",
    "mnist_train_dataset, mnist_val_dataset = random_split(dataset=mnist_dataset, \n",
    "                                                      lengths=[0.9, 0.1])\n",
    "\n",
    "# download the test data\n",
    "mnist_test_dataset = torchvision.datasets.FashionMNIST(root=\"\",\n",
    "                                                  transform=torchvision.transforms.Compose([\n",
    "                                                  torchvision.transforms.ToTensor(),\n",
    "                                                  torchvision.transforms.Normalize((0.5,), (0.5,))]),\n",
    "                                                  train=False,\n",
    "                                                  download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "shuffle = True\n",
    "# train dataloader\n",
    "mnist_train_dataloader = DataLoader(mnist_train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=shuffle)\n",
    "\n",
    "# val dataloader\n",
    "mnist_train_dataloader = DataLoader(mnist_val_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=shuffle)\n",
    "\n",
    "# test dataloader\n",
    "mnist_test_dataloader = DataLoader(mnist_test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot image dependent on the index in the dataset\n",
    "def visualize_picture(index: int) -> None:\n",
    "    img = np.array(mnist_dataset[index][0]).reshape(28, 28)\n",
    "    label = mnist_dataset[index][1]\n",
    "    plt.imshow(img)\n",
    "    plt.title(label)\n",
    "\n",
    "# example of mnist image\n",
    "visualize_picture(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder in class structure\n",
    "class SRAE(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 latent_dim=20):\n",
    "        super().__init__()\n",
    "\n",
    "        # number of latent dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # encoding procedure\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, \n",
    "                      kernel_size=3,\n",
    "                      stride=2, \n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, \n",
    "                      kernel_size=3, \n",
    "                      stride=2, \n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, \n",
    "                      kernel_size=3, \n",
    "                      stride=2, \n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, \n",
    "                      kernel_size=3, \n",
    "                      stride=2, \n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, \n",
    "                      kernel_size=3, \n",
    "                      stride=2, \n",
    "                      padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # decoding procedure\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, \n",
    "                               kernel_size=3, \n",
    "                               stride=2, \n",
    "                               padding=1, \n",
    "                               output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, \n",
    "                               kernel_size=3, \n",
    "                               stride=2, \n",
    "                               padding=1, \n",
    "                               output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, \n",
    "                               kernel_size=3, \n",
    "                               stride=2, \n",
    "                               padding=1, \n",
    "                               output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, \n",
    "                               kernel_size=3, \n",
    "                               stride=2, \n",
    "                               padding=1, \n",
    "                               output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, \n",
    "                               kernel_size=3, \n",
    "                               stride=2, \n",
    "                               padding=1, \n",
    "                               output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        # encode input to latent space\n",
    "        encoded = self.encode(x)\n",
    "        print(encoded.shape)\n",
    "\n",
    "        encoded = encoded.view(encoded.size(0), -1)\n",
    "        latent = encoded[:, :self.latent_dim]\n",
    "        \n",
    "        # decode from latent space\n",
    "        decoded = self.decode(latent.unsqueeze(-1).unsqueeze(-1))\n",
    "        return decoded\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # extract the image from the bact\n",
    "        x, _ = batch\n",
    "\n",
    "        # adding gaussiona noise to the image\n",
    "\n",
    "        # predicted image from AE \n",
    "        x_hat = self(x)\n",
    "\n",
    "        # using mean squared error to calculate \n",
    "        # the distance to the true values\n",
    "        loss = nn.functional.MSE(x_hat, x)\n",
    "\n",
    "        # log the loss for better visualization\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # extract the image from the bact\n",
    "        x, _ = batch\n",
    "\n",
    "        # adding gaussiona noise to the image\n",
    "\n",
    "        # predicted image from AE \n",
    "        x_hat = self(x)\n",
    "\n",
    "        # using mean squared error to calculate \n",
    "        # the distance to the true values\n",
    "        loss = nn.functional.MSE(x_hat, x)\n",
    "\n",
    "        # log the loss for better visualization\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # extract the image from the bact\n",
    "        x, _ = batch\n",
    "\n",
    "        # adding gaussiona noise to the image\n",
    "\n",
    "        # predicted image from AE \n",
    "        x_hat = self(x)\n",
    "\n",
    "        # using mean squared error to calculate \n",
    "        # the distance to the true values\n",
    "        loss = nn.functional.MSE(x_hat, x)\n",
    "\n",
    "        # log the loss for better visualization\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # adam optimizer\n",
    "        optim = Adam(params= self.parameters(),\n",
    "                     lr=1e-3)\n",
    "        return optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/leonrenn/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | encode | Sequential | 392 K \n",
      "1 | decode | Sequential | 392 K \n",
      "--------------------------------------\n",
      "784 K     Trainable params\n",
      "0         Non-trainable params\n",
      "784 K     Total params\n",
      "3.138     Total estimated model params size (MB)\n",
      "/Users/leonrenn/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/188 [00:00<?, ?it/s] torch.Size([32, 256, 1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [256, 128, 3, 3], expected input[32, 20, 1, 1] to have 256 channels, but got 20 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m SRAE()\n\u001b[1;32m      2\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      4\u001b[0m             train_dataloaders\u001b[39m=\u001b[39;49mmnist_train_dataloader)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    518\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 520\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    521\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    550\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    553\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    554\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    555\u001b[0m     ckpt_path,\n\u001b[1;32m    556\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    557\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m )\n\u001b[0;32m--> 559\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    561\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    562\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:978\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m    977\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m--> 978\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    979\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:201\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:354\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(combined_loader)\n\u001b[1;32m    353\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 354\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[1;32m    134\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    135\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py:218\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    217\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], kwargs)\n\u001b[1;32m    219\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:185\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         closure()\n\u001b[1;32m    180\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[1;32m    187\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    188\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:261\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[1;32m    260\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    262\u001b[0m     trainer,\n\u001b[1;32m    263\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    264\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    265\u001b[0m     batch_idx,\n\u001b[1;32m    266\u001b[0m     optimizer,\n\u001b[1;32m    267\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    268\u001b[0m )\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:142\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    141\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    144\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    145\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/core/module.py:1266\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   1228\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1229\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1232\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1233\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1234\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[39m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m \u001b[39m    the optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[39m                    pg[\"lr\"] = lr_scale * self.learning_rate\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1266\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py:158\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[1;32m    162\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:224\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[0;32m--> 224\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(optimizer, model\u001b[39m=\u001b[39;49mmodel, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:114\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/torch/optim/adam.py:121\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 121\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    124\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:101\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[1;32m     90\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     91\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     92\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     93\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     94\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     95\u001b[0m     \u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:308\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[1;32m    307\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    309\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()\n\u001b[1;32m    311\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_result_cls\u001b[39m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[39m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:288\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 288\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    290\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    291\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:366\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[1;32m    365\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TrainingStep)\n\u001b[0;32m--> 366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[29], line 94\u001b[0m, in \u001b[0;36mSRAE.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     89\u001b[0m x, _ \u001b[39m=\u001b[39m batch\n\u001b[1;32m     91\u001b[0m \u001b[39m# adding gaussiona noise to the image\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[39m# predicted image from AE \u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m x_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x)\n\u001b[1;32m     96\u001b[0m \u001b[39m# using mean squared error to calculate \u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m# the distance to the true values\u001b[39;00m\n\u001b[1;32m     98\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mMSE(x_hat, x)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[29], line 84\u001b[0m, in \u001b[0;36mSRAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m latent \u001b[39m=\u001b[39m encoded[:, :\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatent_dim]\n\u001b[1;32m     83\u001b[0m \u001b[39m# decode from latent space\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(latent\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     85\u001b[0m \u001b[39mreturn\u001b[39;00m decoded\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data_augmentation/lib/python3.9/site-packages/torch/nn/modules/conv.py:956\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    951\u001b[0m num_spatial_dims \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    952\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[1;32m    953\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    954\u001b[0m     num_spatial_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose2d(\n\u001b[1;32m    957\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    958\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [256, 128, 3, 3], expected input[32, 20, 1, 1] to have 256 channels, but got 20 channels instead"
     ]
    }
   ],
   "source": [
    "model = SRAE()\n",
    "trainer = Trainer(max_epochs=5)\n",
    "trainer.fit(model=model,\n",
    "            train_dataloaders=mnist_train_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_augmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
